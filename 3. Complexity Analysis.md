# Complexity Analysis

### Time complexity
```
Time complexity != Time Taken to run the algorithm
```
- It is the function that gives us how time grows when size of the input grows.
- O(1) is known the Constant Time complexity.

### What do we consider when thinking about complexity
1. Always look for worst case complexity.
2. Always look ar the complexity for large infinte data.
3. (i) We don't care about actual time.

   (ii) We ignore all constants because we only consider how time grows when input grows.

4. Always ignore less dominating terms.

### Example
O(3N<sup>3</sup>+4N<sup>2</sup>+5N+6)

Apply Rule 3(Ignore constants). So, (N<sup>3</sup>+N<sup>2</sup>+N)

Then apply Rule 4(Ignore less dominating terms)

Therefore, O(N<sup>3</sup>) is the Time complexity

### Big O notation
- It gives the upper bound values and worst case complexity.
- In the above example, the program will not take more than N<sup>3</sup> time to execute.
- The math behind Big O notation is 

![Big%20O%20notation](https://github.com/Harini-Pavithra/Data-Structures/blob/main/Images/Big%20O%20notation.PNG)

### Big Omega notation
- It's the opposite of Big O notation.
- It gives the lower bound values.
- For the above example, it will take atleast Ω(N<sup>3</sup>) times to execute.
- The math behind Big Omega notation is 

![Big%20Omega%20notation](https://github.com/Harini-Pavithra/Data-Structures/blob/main/Images/Big%20Omega%20notation.jpg)

### Theta notation
- It's the combination of Big O and Big Omega notations(upper and lower bound values)
- It is represented using Θ().
- The math behind Theta notation is 

![Theta%20notations](https://github.com/Harini-Pavithra/Data-Structures/blob/main/Images/Theta%20notations.jpg)

### Little o notation
- Little-ο notation is used to describe an upper-bound value.
- It is represented using ο()
- The math behind Little o notation is

![Little%20o%20notation](https://github.com/Harini-Pavithra/Data-Structures/blob/main/Images/Little%20o%20notation.jpg)

### Little omega notation
- Little-οmega notation is used to describe an lower-bound value. 
- It is represented using ω()
- The math behind Little omega notation is

![Little%20Omega%20notation](https://github.com/Harini-Pavithra/Data-Structures/blob/main/Images/Little%20Omega%20notation.jpg)

### Why do we need different notations to represent complexities?
- It's impossible to tell these algorithm complexity in seconds or milliseconds.
- And that's why we need different notations to represent complexities of algorithms.
- Big O notation is one of the commonly used notations to find complexities.

### Constant Time complexity
- O(1) is known as the Constant Time complexity.
- What it is this 1 referring? As Asymptatic analysis point of view, we just don't write the number of operations. We just write it as constant i.e O(1)
- Accessing a single memory slot would take constant time.

### Complexity Analysis Graph

![Complexity_Analysis_Graph](https://github.com/Harini-Pavithra/Data-Structures/blob/main/Images/Complexity_Analysis_Graph.PNG)

- There are lot of runtime worse than O(N!) too such as O(N<sup>N</sup>) or O(2<sup>N</sup>*N!)

### Examples
1. O(M<sup>2</sup>+2N)

After ignoring the constants, we will have O(M<sup>2</sup>+N)

We can't ignore M and N, since N is not the same function of M.

2. O(N!+log(M)+2N+3)

After ignoring the constants and less dominating terms, we will have O(N!+log(M))

### Space complexity
- It is the memory required by an algorithm until it executes completely.
- The speed of the algorithm depends on the size of the input

### Logarithms 

![]()

- One of the classic examples of logarithms is Binary Search tree.

### Tips
- If the half of the input is eliminated at every step of the function and 
- If we double the size of input and only one extra operation need to be performed to reduce it then the complexity is O(log N)

### Recursion
- Space complexity of recursion problems = Height of the tree (i.e path of the tree)
- The 2 types of recursion problem is
  1. Linear recursion
  2. Divide and Conquer
- The Divide and Conquer form is 

![Divide%20and%20Conquer](https://github.com/Harini-Pavithra/Data-Structures/blob/main/Images/Divide%20and%20Conquer.jpg)

### How to actually solve to get the complexity?
1. Plug-and-chug method
  - Plug-and-chug is another way to solve recurrences. 
  - This is also sometimes called “expansion” or “iteration”.
  -  As in guess-and-verify, the key step is identifying a pattern.
2. Master methos
  - The master method is a formula for solving recurrence relations of the form:
  ```
  T(n) = aT(n/b) + f(n), where, n = size of input a = number of subproblems in the recursion n/b = size of each subproblem. All subproblems are assumed to have the same size.

  ```
 3. Akra–Bazzi method
   - the Akra–Bazzi method is used to analyze the asymptotic behavior of the mathematical recurrences that appear in the analysis of divide and conquer algorithms where the sub-problems have substantially different sizes.
 
 ![Akra%20Bazzi](https://github.com/Harini-Pavithra/Data-Structures/blob/main/Images/Akra%20Bazzi.jpg)
 
 ### Note
 if p < power of (g(n))
 
 then answer = g(n) i.e O(g(n))
 
 ### Solving Linear Recurrences
 - For example, f(N) = f(n-1)+f(n-2) then the form takes
 
 ![Linear%20recurrence](https://github.com/Harini-Pavithra/Data-Structures/blob/main/Images/Linear%20recurrence.jpg)
 
 ### Fibonacci number 
 - Homogeneous Linear occurrences formula for nth fibonacci number

![Homogeneous](https://github.com/Harini-Pavithra/Data-Structures/blob/main/Images/Homogeneous.jpg)

![Time%20complexity](https://github.com/Harini-Pavithra/Data-Structures/blob/main/Images/Time%20complexity.jpg)

- Non-homogeneous Linear occurrences formula for nth fibonacci number
 
 ![Non-homogeneous](https://github.com/Harini-Pavithra/Data-Structures/blob/main/Images/Non-homogeneous.jpg)
 
 ### Tip
 - Sorting each string takes O(N logN) time complexity.
